/* Three-stage parallel scan */

/* The kernel should be called defining TYPE as the input/output data type,
   OP(acc, el) as the operator and OP_NULL as the null item for the operator
   (e.g. 0 for sum).
*/

#ifndef TYPE
#define TYPE float
#endif

/* Token joiner for the C preprocessor */
#define _TOK(a, b) a##b
#define TOK(a, b) _TOK(a, b)

#ifndef FMIN_OP /* TODO, not supported yet */
#define FMIN_OP(acc, el) acc = fmin(acc, el)
#define FMIN_OP_NULL NAN
#endif

#ifndef ADD_OP
#define ADD_OP(acc, el) acc += el
#define ADD_OP_NULL 0
#endif

#if !defined(OP)
#error "Please define an OP"
#endif

#if !defined(OP_NULL)
#error "Please define an OP_NULL"
#endif

/* The VECSIZE is the vector width of the data type when reading from global memory.
   A VECSIZE of 1 means that each work-item reads elements one at a time, while
   e.g. a VECSIZE of 4 means that (if the datatype is int) each work-item reads
   an int4 at a time, while still reducing everythig to single ints. */

#define _LASTOF2 1
#define _LASTOF4 3
#define _LASTOF8 7
#define _LASTOF16 f

#define VECSCAN2(acc) OP(acc.s1, acc.s0)
#define VECSCAN4(acc) VECSCAN2(acc); \
	OP(acc.s2, acc.s1); \
	OP(acc.s3, acc.s2)
#define VECSCAN8(acc) VECSCAN4(acc); \
	OP(acc.s4, acc.s3); \
	OP(acc.s5, acc.s4); \
	OP(acc.s6, acc.s5); \
	OP(acc.s7, acc.s6)
#define VECSCAN16(acc) VECSCAN8(acc); \
	OP(acc.s8, acc.s7); \
	OP(acc.s9, acc.s8); \
	OP(acc.sa, acc.s9); \
	OP(acc.sb, acc.sa); \
	OP(acc.sc, acc.sb); \
	OP(acc.sd, acc.sc); \
	OP(acc.se, acc.sd); \
	OP(acc.sf, acc.se)

/* TODO: define VECREDUCE based on OP */
#if VECSIZE == 1
#define VECTYPE TYPE
#define VECSCAN(acc) /* nothing */
#define LASTOF(acc) acc
#else
#if VECSIZE > 16 || (VECSIZE & (VECSIZE - 1))
#error "Only vector sizes of 2, 4, 8 and 16 are supported"
#else
#define VECTYPE TOK(TYPE, VECSIZE)
#define VECSCAN(acc) TOK(VECSCAN, VECSIZE)(acc)
#define LASTOF(acc) acc.TOK(s, TOK(_LASTOF, VECSIZE))
#endif
#endif

TYPE local_scan(
	__global VECTYPE * restrict output,
	__global const VECTYPE * restrict input,
	__local TYPE * lmem,
	const uint numvecels, /* number of VECTOR elements */
	const uint offset,
	const uint last_offset)
{
	TYPE reduction_prev_pass = 0;
	VECTYPE data = (VECTYPE)(OP_NULL);

	const int li = get_local_id(0);
	const int lws = get_local_size(0);

	/* index in global memory */
	uint gi = get_local_id(0) + offset;
	const uint gi_end = get_local_id(0) + last_offset;

	while (gi < gi_end) {
		/* load from gmem, microscan */
		if (gi < numvecels) {
			data = input[gi];
			VECSCAN(data);
		} else {
			data = (VECTYPE)(0);
		}

		/* scan the data 'tails' */

		TYPE tail = LASTOF(data);

		/* load in lmem */
		barrier(CLK_LOCAL_MEM_FENCE);
		lmem[li] = tail;

		uint stride = 1;
		const uint stride_end = lws/2;

		while (stride <= stride_end) {
			barrier(CLK_LOCAL_MEM_FENCE);
			if (li >= stride)
				OP(tail, lmem[li - stride]);
			barrier(CLK_LOCAL_MEM_FENCE);
			lmem[li] = tail;
			stride *= 2;
		}

		/* augment data with the result of the scan of previous microscan residues */
		barrier(CLK_LOCAL_MEM_FENCE);
		if (li > 0)
			OP(data, (VECTYPE)(lmem[li - 1]));
		/* and the residue from the previous pass */
		OP(data, (VECTYPE)(reduction_prev_pass));

		if (gi < numvecels)
			output[gi] = data;

		/* augment the residual of the previous pass by the residual of this pass */
		reduction_prev_pass += lmem[lws - 1];
		gi += lws;
	}

	return reduction_prev_pass;
}

/* first step: each work-group scans a chunk of els_per_wg vector elements,
 * storing the last vaue (the reduction) in an auxiliary array */
__kernel
void first_scan_step(
	/* output buffer */
	__global VECTYPE * restrict scan,
	/* aux buffer */
	__global TYPE * restrict aux,
	/* input buffer */
	__global const VECTYPE * restrict v1,
	/* local storage for scan */
	__local TYPE * lmem,
	uint numels, uint els_per_wg)
{
	const uint offset = els_per_wg*get_group_id(0)/VECSIZE;
	const uint last_offset = offset + els_per_wg/VECSIZE;

	TYPE reduction_prev_pass = local_scan(scan, v1, lmem, numels/VECSIZE, offset, last_offset);

	if (get_local_id(0) == 0)
		aux[get_group_id(0)] = reduction_prev_pass;
}

/* scan the auxiliary array with a single work-group */
__kernel
void aux_scan_step(__global VECTYPE * restrict aux,
	__local TYPE * lmem, uint numels)
{
	const uint li = get_local_id(0);
	const uint lws = get_local_size(0);

	const uint offset = 0;
	const uint last_offset = offset + numels/VECSIZE;

	local_scan(aux, aux, lmem, numels/VECSIZE, offset, last_offset);
}


/* add the scan of the reductions of the chunks of the first pass
   to each element. This is launched for one group less than the first pass,
   since elements in the first group do not need any correction.
   Note that this kernel must be launched with the same grid config as the first pass.
 */
__kernel
void last_scan_step(__global VECTYPE * restrict scan,
	__global const TYPE * restrict aux,
	uint numels, uint els_per_wg)
{
	const int prev_chunk = get_group_id(0);
	const int this_chunk = prev_chunk + 1;

	const VECTYPE correction = (VECTYPE)(aux[prev_chunk]);

	uint gi = get_local_id(0) + els_per_wg*this_chunk/VECSIZE;

	const uint gi_end = min(gi + els_per_wg/VECSIZE, numels/VECSIZE);

	while (gi < gi_end) {
		/* correct the element */
		scan[gi] += correction;

		gi += get_local_size(0);
	}
}

